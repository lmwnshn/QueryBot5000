{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "3fa725de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics.pairwise\n",
    "import sklearn.neighbors\n",
    "import sklearn.preprocessing\n",
    "import sqlparse\n",
    "\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aada6351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/extracted/simple/postgresql-2021-12-06_160118.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160210.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160202.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160154.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160132.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160149.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160207.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160205.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160121.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160048.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160138.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160127.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160146.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160135.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160129.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160124.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160151.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160157.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160159.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160140.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160143.csv',\n",
       " 'data/extracted/simple/postgresql-2021-12-06_160114.csv']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pgfiles = glob.glob('data/extracted/simple/postgresql*.csv')\n",
    "display(pgfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9037dbb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(603434, 4)\n",
      "Index(['log_time', 'command_tag', 'session_start_time', 'message'], dtype='object')\n",
      "{nan, 'UPDATE', 'INSERT', 'SELECT', 'ROLLBACK', 'SHOW', 'COMMIT', 'BEGIN', 'SET', 'DELETE'}\n"
     ]
    }
   ],
   "source": [
    "# https://www.postgresql.org/docs/13/runtime-config-logging.html#RUNTIME-CONFIG-LOGGING-CSVLOG\n",
    "PG_LOG_COLUMNS = [\n",
    "    'log_time',\n",
    "    'user_name',\n",
    "    'database_name',\n",
    "    'process_id',\n",
    "    'connection_from',\n",
    "    'session_id',\n",
    "    'session_line_num',\n",
    "    'command_tag',\n",
    "    'session_start_time',\n",
    "    'virtual_transaction_id',\n",
    "    'transaction_id',\n",
    "    'error_severity',\n",
    "    'sql_state_code',\n",
    "    'message',\n",
    "    'detail',\n",
    "    'hint',\n",
    "    'internal_query',\n",
    "    'internal_query_pos',\n",
    "    'context',\n",
    "    'query',\n",
    "    'query_pos',\n",
    "    'location',\n",
    "    'application_name',\n",
    "    'backend_type',\n",
    "]\n",
    "\n",
    "\n",
    "df = pd.concat(\n",
    "    pd.read_csv(pgfile,\n",
    "                names=PG_LOG_COLUMNS,\n",
    "                parse_dates=['log_time', 'session_start_time'],\n",
    "                usecols=['log_time', 'session_start_time', 'command_tag', 'message'],\n",
    "                header=None,\n",
    "                index_col=False)\n",
    "    for pgfile in pgfiles\n",
    ")\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(set(df['command_tag']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e366a",
   "metadata": {},
   "source": [
    "## Extracting the relevant queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c9adcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        UPDATE stock   SET S_QUANTITY = 38 ,        S_...\n",
       "1        UPDATE stock   SET S_QUANTITY = 76 ,        S_...\n",
       "2        UPDATE stock   SET S_QUANTITY = 43 ,        S_...\n",
       "3        UPDATE stock   SET S_QUANTITY = 73 ,        S_...\n",
       "4        UPDATE stock   SET S_QUANTITY = 89 ,        S_...\n",
       "                               ...                        \n",
       "28203    INSERT INTO order_line (OL_O_ID, OL_D_ID, OL_W...\n",
       "28204    INSERT INTO order_line (OL_O_ID, OL_D_ID, OL_W...\n",
       "28205    UPDATE stock   SET S_QUANTITY = 89 ,        S_...\n",
       "28206    UPDATE stock   SET S_QUANTITY = 43 ,        S_...\n",
       "28207    UPDATE stock   SET S_QUANTITY = 47 ,        S_...\n",
       "Name: query, Length: 603434, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commands = ['SELECT', 'INSERT', 'UPDATE', 'DELETE']\n",
    "\n",
    "def extract_query(message):\n",
    "    for command in commands:\n",
    "        idx = message.find(command)\n",
    "        if idx != -1:\n",
    "            query = message[idx:]\n",
    "            return query\n",
    "    return ''\n",
    "\n",
    "df['query'] = df['message'].apply(extract_query)\n",
    "df['query']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa46e16",
   "metadata": {},
   "source": [
    "## Anonymizer: salt and hash non-date non-digit strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaae2070",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANONYMIZE = False\n",
    "\n",
    "SALT = 'andycannotsay.com'.encode('utf-8')\n",
    "DATE_REGEX = re.compile(r'\\d{4}-\\d{2}-\\d{2}.*')\n",
    "DIGITS_REGEX = re.compile(r'\\d+\\.?\\d*')\n",
    "\n",
    "def anonymize(sql):\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    # TODO(WAN): sqlparse.parse is actually quite slow.\n",
    "    # Do we really need this?\n",
    "    parsed = sqlparse.parse(sql)\n",
    "    if len(parsed) == 0:\n",
    "        return ''\n",
    "    \n",
    "    assert len(parsed) == 1\n",
    "    tokens = parsed[0].flatten()\n",
    "    for token in tokens:\n",
    "        token = str(token)\n",
    "\n",
    "        single_quoted = token.startswith(\"'\") and token.endswith(\"'\")\n",
    "        double_quoted = token.startswith('\"') and token.endswith('\"')\n",
    "        not_quoted = not single_quoted and not double_quoted\n",
    "\n",
    "        is_date = DATE_REGEX.search(token) is not None\n",
    "        is_digits = DIGITS_REGEX.search(token) is not None\n",
    "\n",
    "        if not_quoted or is_date or is_digits:\n",
    "            cleaned_tokens.append(token)\n",
    "            continue\n",
    "\n",
    "        sha = hashlib.sha256(SALT + token.encode('utf-8')).hexdigest()\n",
    "        clean_token = \"'{}\\\\{}'\".format(len(token) - 2, sha)\n",
    "        cleaned_tokens.append(clean_token)\n",
    "\n",
    "    return ''.join(cleaned_tokens)\n",
    "\n",
    "if ANONYMIZE:\n",
    "    df['query_anon'] = df['query'].apply(anonymize)\n",
    "    df['query_anon']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477f3c71",
   "metadata": {},
   "source": [
    "## Pre-processor: extracting query templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82d93be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        UPDATE stock   SET S_QUANTITY = # ,        S_Y...\n",
       "1        UPDATE stock   SET S_QUANTITY = # ,        S_Y...\n",
       "2        UPDATE stock   SET S_QUANTITY = # ,        S_Y...\n",
       "3        UPDATE stock   SET S_QUANTITY = # ,        S_Y...\n",
       "4        UPDATE stock   SET S_QUANTITY = # ,        S_Y...\n",
       "                               ...                        \n",
       "28203    INSERT INTO order_line (OL_O_ID, OL_D_ID, OL_W...\n",
       "28204    INSERT INTO order_line (OL_O_ID, OL_D_ID, OL_W...\n",
       "28205    UPDATE stock   SET S_QUANTITY = # ,        S_Y...\n",
       "28206    UPDATE stock   SET S_QUANTITY = # ,        S_Y...\n",
       "28207    UPDATE stock   SET S_QUANTITY = # ,        S_Y...\n",
       "Name: query_template, Length: 603434, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STRING_REGEX = r'([^\\\\])\\'((\\')|(.*?([^\\\\])\\'))'\n",
    "DOUBLE_QUOTE_STRING_REGEX = r'([^\\\\])\"((\")|(.*?([^\\\\])\"))'\n",
    "INT_REGEX = r'([^a-zA-Z])-?\\d+(\\.\\d+)?'\n",
    "HASH_REGEX = r'(\\'\\d+\\\\.*?\\')'\n",
    "\n",
    "def extract_template(query):\n",
    "    template = query\n",
    "    template = re.sub(HASH_REGEX, r\"@@@\", template)\n",
    "    template = re.sub(STRING_REGEX, r\"\\1&&&\", template)\n",
    "    template = re.sub(DOUBLE_QUOTE_STRING_REGEX, r\"\\1&&&\", template)\n",
    "    template = re.sub(INT_REGEX, r\"\\1#\", template)\n",
    "    return template\n",
    "\n",
    "query_column = 'query_anon' if ANONYMIZE else 'query'\n",
    "\n",
    "df['query_template'] = df[query_column].apply(extract_template)\n",
    "df['query_template']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cb57547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2021-12-06 16:01:18-05:00\n",
       "1       2021-12-06 16:01:18-05:00\n",
       "2       2021-12-06 16:01:18-05:00\n",
       "3       2021-12-06 16:01:18-05:00\n",
       "4       2021-12-06 16:01:18-05:00\n",
       "                   ...           \n",
       "28203   2021-12-06 16:01:18-05:00\n",
       "28204   2021-12-06 16:01:18-05:00\n",
       "28205   2021-12-06 16:01:18-05:00\n",
       "28206   2021-12-06 16:01:18-05:00\n",
       "28207   2021-12-06 16:01:18-05:00\n",
       "Name: log_time_s, Length: 603434, dtype: datetime64[ns, pytz.FixedOffset(-300)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['log_time_s'] = df['log_time'].round('S')\n",
    "df['log_time_s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ba90ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_template</th>\n",
       "      <th>log_time_s</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">DELETE FROM new_order WHERE NO_O_ID = #    AND NO_D_ID = #   AND NO_W_ID = #</th>\n",
       "      <th>2021-12-06 16:01:12-05:00</th>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-06 16:01:13-05:00</th>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-06 16:01:14-05:00</th>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-06 16:01:15-05:00</th>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-06 16:01:16-05:00</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">UPDATE warehouse   SET W_YTD = W_YTD + #  WHERE W_ID = #</th>\n",
       "      <th>2021-12-06 16:02:08-05:00</th>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-06 16:02:09-05:00</th>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-06 16:02:10-05:00</th>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-06 16:02:11-05:00</th>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-06 16:02:12-05:00</th>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1929 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              count\n",
       "query_template                                     log_time_s                      \n",
       "DELETE FROM new_order WHERE NO_O_ID = #    AND ... 2021-12-06 16:01:12-05:00    110\n",
       "                                                   2021-12-06 16:01:13-05:00    158\n",
       "                                                   2021-12-06 16:01:14-05:00     75\n",
       "                                                   2021-12-06 16:01:15-05:00    137\n",
       "                                                   2021-12-06 16:01:16-05:00     90\n",
       "...                                                                             ...\n",
       "UPDATE warehouse   SET W_YTD = W_YTD + #  WHERE... 2021-12-06 16:02:08-05:00    159\n",
       "                                                   2021-12-06 16:02:09-05:00    174\n",
       "                                                   2021-12-06 16:02:10-05:00    161\n",
       "                                                   2021-12-06 16:02:11-05:00    169\n",
       "                                                   2021-12-06 16:02:12-05:00    126\n",
       "\n",
       "[1929 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = df.groupby(['query_template', 'log_time_s']).size()\n",
    "grouped_df = pd.DataFrame(gb, columns=['count'])\n",
    "grouped_df.drop('', axis=0, level=0, inplace=True)\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa6f4bc",
   "metadata": {},
   "source": [
    "## Clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5832cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(WAN): Port online_clustering.py.\n",
    "# TODO(WAN): I would be somewhat surprised if sklearn doesn't have this built in... We'll see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "43ff90cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = grouped_df.copy()\n",
    "assert DF.index.names == ['query_template', 'log_time_s']\n",
    "assert DF.columns.values == ['count']\n",
    "\n",
    "min_time = DF.index.get_level_values(1).min()\n",
    "max_time = DF.index.get_level_values(1).max()\n",
    "\n",
    "rho = 0.8\n",
    "cluster_gap = 1440\n",
    "n = (max_time - min_time).seconds // 60 + (max_time - min_time).days * 1440 + 1\n",
    "n_gaps = n // cluster_gap\n",
    "\n",
    "centers : Dict[int, pd.DataFrame] = {}\n",
    "cluster_totals : Dict[int, int] = {}\n",
    "cluster_sizes : Dict[int, int] = {}\n",
    "\n",
    "assignments = [(min_time, {template: None for template in sorted(DF.index.get_level_values(0))})]\n",
    "\n",
    "current_time = min_time\n",
    "next_cluster = 0\n",
    "\n",
    "def Update(cluster, template, timestamp_low, timestamp_high):\n",
    "    df = grouped_df.query(\n",
    "        \"`query_template` == @template\"\n",
    "        \" and @timestamp_low <= `log_time_s`\"\n",
    "        \" and `log_time_s` < @timestamp_high\")\n",
    "    cluster_totals[cluster] += df.sum()\n",
    "    raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "25a4d975",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'next_cluster' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4534/4112626758.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m \u001b[0mAdjustCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;31m#[datetime.timedelta(seconds=offset) for offset in offsets] + min_date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4534/4112626758.py\u001b[0m in \u001b[0;36mAdjustCluster\u001b[0;34m(start_time, current_time, next_time)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0massignment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_cluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mcenters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_cluster\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cluster{next_cluster}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mcluster_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_cluster\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'next_cluster' referenced before assignment"
     ]
    }
   ],
   "source": [
    "DF = grouped_df.copy()\n",
    "\n",
    "min_time = DF.index.get_level_values(1).min()\n",
    "max_time = DF.index.get_level_values(1).max()\n",
    "\n",
    "rho = 0.8\n",
    "cluster_gap = 1440\n",
    "n = (max_time - min_time).seconds // 60 + (max_time - min_time).days * 1440 + 1\n",
    "num_gaps = n // cluster_gap\n",
    "\n",
    "centers, cluster_totals, cluster_sizes = {}, {}, {}\n",
    "\n",
    "assignments = [(min_time, {template: None for template in sorted(DF.index.get_level_values(0))})]\n",
    "\n",
    "current_time = min_time\n",
    "next_cluster = 0\n",
    "\n",
    "\n",
    "def Similarity(template1, template2, timestamps):\n",
    "    # Query the data for the counts.\n",
    "    df1 = grouped_df.query(\n",
    "        \"`query_template` == @template1\"\n",
    "        \" and `log_time_s` in @timestamps\")\n",
    "    df2 = grouped_df.query(\n",
    "        \"`query_template` == @template2\"\n",
    "        \" and `log_time_s` in @timestamps\")\n",
    "    # Reshape because we only have a single feature, the count.\n",
    "    arr1 = df1.values.reshape(1, -1)\n",
    "    arr2 = df2.values.reshape(1, -1)\n",
    "    # Compute the cosine similarity.\n",
    "    return sklearn.metrics.pairwise.cosine_similarity(arr1, arr2)[0][0]\n",
    "\n",
    "def ExtractSample(template, timestamps : pd.DatetimeIndex):\n",
    "    df = grouped_df.query(\n",
    "        \"`query_template` == @template\"\n",
    "        \" and `log_time_s` in @timestamps\"\n",
    "        ).droplevel(0)\n",
    "    df = df.reindex(index=timestamps, fill_value=0)\n",
    "    return df['count'].values\n",
    "\n",
    "def Update(cluster, template, timestamp_low, timestamp_high):\n",
    "    df = grouped_df.query(\n",
    "        \"`query_template` == @template\"\n",
    "        \" and @timestamp_low <= `log_time_s`\"\n",
    "        \" and `log_time_s` < @timestamp_high\")\n",
    "    cluster_totals[cluster] += df.sum()\n",
    "    raise Exception\n",
    "\n",
    "    \n",
    "def SampleTimestamps(start_time, n):\n",
    "    n_sample = 10000\n",
    "    if n > n_sample:\n",
    "        offsets = np.random.choice(a=n, size=n_sample, replace=False)\n",
    "    else:\n",
    "        offsets = np.arange(n)\n",
    "    timestamps = start_time + pd.array([pd.Timedelta(seconds=offset) for offset in offsets])\n",
    "    return timestamps\n",
    "\n",
    "def AdjustCluster(start_time, current_time, next_time):\n",
    "    timestamps = SampleTimestamps(start_time, n)\n",
    "    \n",
    "    last_assignment = assignments[-1][1]\n",
    "    assignment = last_assignment.copy()\n",
    "    for cluster in centers.keys():\n",
    "        for template in last_assignment:\n",
    "            if last_assignment[template] == cluster:\n",
    "                Update(cluster, template, current_time, next_time)\n",
    "                \n",
    "    # kd-tree construction.\n",
    "    samples = np.array([\n",
    "        ExtractSample(centers[cluster], timestamps)\n",
    "        for cluster in sorted(centers.keys())\n",
    "    ])\n",
    "    if len(samples) == 0:\n",
    "        neighbors = None\n",
    "    else:\n",
    "        normalized_samples = sklearn.preprocessing.normalize(samples, copy=False)\n",
    "        neighbors = sklearn.neighbors.NearestNeighbors(n_neighbors=1, algorithm='kd_tree', metric='l2')\n",
    "        neighbors.fit(normalized_samples)\n",
    "    \n",
    "    # Initial assignments.\n",
    "    for template in sorted(set(grouped_df.index.get_level_values(0))):\n",
    "        if assignment[template] is not None:\n",
    "            cluster = assignment[template]\n",
    "            print(f'Start {cluster}: {template}')\n",
    "            center = centers[cluster]\n",
    "            # If the template is the only template in its cluster, or still belongs, continue.\n",
    "            if cluster_sizes[cluster] == 1 or Similarity(template, center, timestamps) > rho:\n",
    "                continue\n",
    "            # Otherwise, reassign the template.\n",
    "            cluster_sizes[cluster] -= 1\n",
    "            Update(cluster, template, min_time, next_time)\n",
    "        \n",
    "        if assignment[template] is None:\n",
    "            first_arrival = grouped_df.xs(template, level=0).index.min()\n",
    "            if first_arrival > current_time:\n",
    "                continue\n",
    "        \n",
    "        new_cluster = None\n",
    "        # Try to assign to an existing cluster.\n",
    "        if neighbors == None:\n",
    "            for cluster in centers.keys():\n",
    "                if Similarity(template, centers[cluster], index) > rho:\n",
    "                    new_cluster = cluster\n",
    "                    break\n",
    "        else:\n",
    "            #neighbor = neighbors.kneighbors(...)\n",
    "            pass\n",
    "        \n",
    "        if new_cluster != None:\n",
    "            pass\n",
    "        \n",
    "        assignment[template] = next_cluster\n",
    "        centers[next_cluster] = 'cluster{next_cluster}'\n",
    "        cluster_sizes[next_cluster] = 1\n",
    "\n",
    "        next_cluster += 1\n",
    "        raise Exception\n",
    "        \n",
    "    # UF\n",
    "    clusters = list(centers.keys())\n",
    "    root = [-1] * len(clusters)\n",
    "    \n",
    "    # kd-tree.\n",
    "    samples = np.array([\n",
    "        ExtractSample(centers[cluster], timestamps)\n",
    "        for cluster in clusters\n",
    "    ])\n",
    "    \n",
    "    if len(samples) == 0:\n",
    "        neighbors = None\n",
    "    else:\n",
    "        normalized_samples = sklearn.preprocessing.normalize(samples, copy=False)\n",
    "        neighbors = sklearn.neighbors.NearestNeighbors(n_neighbors=2, algorithm='kd_tree', metric='l2')\n",
    "        neighbors.fit(normalized_samples)\n",
    "    \n",
    "    # Merge clusters.\n",
    "    for cluster in clusters:\n",
    "        pass\n",
    "            \n",
    "        \n",
    "\n",
    "# AdjustCluster(min_date, min_date, min_date)\n",
    "        \n",
    "#[datetime.timedelta(seconds=offset) for offset in offsets] + min_date\n",
    "#min_date + offsets.astype('datetime64[s]')[0]\n",
    "\n",
    "# def AddToCenter(center, lower, upper, data, positive):\n",
    "#     grouped_df.groupby(['query_template', 'log_time_s'])\n",
    "\n",
    "# def AddToCenter(center, ts_low, ts_high, template, positive=True):\n",
    "\n",
    "\n",
    "# ts_low = '2021-12-06 16:01:12-05:00'\n",
    "# ts_high = '2021-12-06 16:01:15-05:00'\n",
    "\n",
    "# template = 'DELETE FROM new_order WHERE NO_O_ID = #    AND NO_D_ID = #   AND NO_W_ID = #'\n",
    "# template2 = 'SELECT OL_I_ID, OL_SUPPLY_W_ID, OL_QUANTITY, OL_AMOUNT, OL_DELIVERY_D   FROM order_line WHERE OL_O_ID = #   AND OL_D_ID = #   AND OL_W_ID = #'\n",
    "# timestamps = pd.DatetimeIndex(['2021-12-06 16:01:12-05:00', '2021-12-09 16:01:13-05:00'])\n",
    "# grouped_df.query('`query_template` == @template and @ts_low <= `log_time_s` and `log_time_s` < @ts_high ')\n",
    "\n",
    "# AdjustCluster(month_min_date, current_date, next_date, data, assignments[-1][1],\n",
    "#                next_cluster, centers, cluster_totals, total_queries, cluster_sizes, rho)\n",
    "\n",
    "# def AdjustCluster(min_date, current_date, next_date, data, last_ass,\n",
    "#        next_cluster, centers, cluster_totals, total_queries, cluster_sizes, rho):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dd71d8",
   "metadata": {},
   "source": [
    "## Forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee20bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(WAN): Port exp_multi_online_continuous.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
